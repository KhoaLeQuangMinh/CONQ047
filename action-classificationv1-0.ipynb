{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":125907,"databundleVersionId":14910023,"sourceType":"competition"}],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport os\nimport wandb\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom torchvision.transforms import InterpolationMode\nimport torchvision.transforms.functional as TF\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nimport timm\nimport random\nfrom torch.utils.data import random_split\nimport torchvision.models as models\nimport pytorch_lightning as pl\nimport torchmetrics\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nfrom pytorch_lightning.loggers import WandbLogger\n\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {DEVICE}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:08:19.045537Z","iopub.execute_input":"2026-01-10T13:08:19.045815Z","iopub.status.idle":"2026-01-10T13:08:19.051648Z","shell.execute_reply.started":"2026-01-10T13:08:19.045796Z","shell.execute_reply":"2026-01-10T13:08:19.050876Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"# Data paths\nPATH_DATA_TRAIN = r'/kaggle/input/action-video/data/data_train'\nPATH_DATA_TEST = r'/kaggle/input/action-video/data/test'\n\n# Model parameters \nNUM_FRAMES = 16\nFRAME_STRIDE = 2\nIMG_SIZE = 224\n\n# Training parameters\nBATCH_SIZE = 16 \nEPOCHS = 16 \nBASE_LR = 1e-4\nHEAD_LR = 5e-5\nWEIGHT_DECAY = 1e-4\nGRAD_ACCUM_STEPS = 4\n\n# PRETRAINED_NAME = 'vit_small_patch16_224'\nPRETRAINED_NAME = 'MCG-NJU/videomae-base'\ngenerator = torch.Generator().manual_seed(42)\n    \nprint(f\"Train data: {PATH_DATA_TRAIN}\")\nprint(f\"Test data: {PATH_DATA_TEST}\")\nprint(f\"Model: {PRETRAINED_NAME}\")\nprint(f\"Frames per video: {NUM_FRAMES}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Epochs: {EPOCHS}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:38:19.500390Z","iopub.execute_input":"2026-01-10T12:38:19.501169Z","iopub.status.idle":"2026-01-10T12:38:19.506656Z","shell.execute_reply.started":"2026-01-10T12:38:19.501141Z","shell.execute_reply":"2026-01-10T12:38:19.505898Z"}},"outputs":[{"name":"stdout","text":"Train data: /kaggle/input/action-video/data/data_train\nTest data: /kaggle/input/action-video/data/test\nModel: MCG-NJU/videomae-base\nFrames per video: 16\nBatch size: 16\nEpochs: 16\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"class VideoTransform:\n    def __init__(self, image_size=224, is_train=True):\n        self.image_size = image_size\n        self.is_train = is_train\n        self.mean = [0.485, 0.456, 0.406]\n        self.std = [0.229, 0.224, 0.225]\n    \n    def __call__(self, frames):\n        if self.is_train:\n            h, w = frames.shape[-2:]\n            scale = random.uniform(0.8, 1.0)\n            new_h, new_w = int(h * scale), int(w * scale)\n            frames = TF.resize(frames, [new_h, new_w], interpolation=InterpolationMode.BILINEAR)\n            i = random.randint(0, max(0, new_h - self.image_size))\n            j = random.randint(0, max(0, new_w - self.image_size))\n            frames = TF.crop(frames, i, j, min(self.image_size, new_h), min(self.image_size, new_w))\n            frames = TF.resize(frames, [self.image_size, self.image_size], interpolation=InterpolationMode.BILINEAR)\n            if random.random() < 0.5:\n                frames = TF.hflip(frames)\n        else:\n            frames = TF.resize(frames, [self.image_size, self.image_size], interpolation=InterpolationMode.BILINEAR)\n        normalized = [TF.normalize(frame, self.mean, self.std) for frame in frames]\n        return torch.stack(normalized)\n\nprint(\"Augmentation defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:16:27.811044Z","iopub.execute_input":"2026-01-10T12:16:27.811736Z","iopub.status.idle":"2026-01-10T12:16:27.818629Z","shell.execute_reply.started":"2026-01-10T12:16:27.811708Z","shell.execute_reply":"2026-01-10T12:16:27.817912Z"}},"outputs":[{"name":"stdout","text":"Augmentation defined\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"class VideoDataset(Dataset):\n    def __init__(self, root, num_frames=16, frame_stride=2, image_size=224, is_train=True):\n        self.root = Path(root)\n        self.num_frames = num_frames\n        self.frame_stride = frame_stride\n        self.transform = VideoTransform(image_size, is_train)\n        self.to_tensor = transforms.ToTensor()\n        self.classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])\n        self.class_to_idx = {name: idx for idx, name in enumerate(self.classes)}\n        self.samples = []\n        for cls in self.classes:\n            cls_dir = self.root / cls\n            for video_dir in sorted([d for d in cls_dir.iterdir() if d.is_dir()]):\n                frame_paths = sorted([p for p in video_dir.iterdir() if p.suffix.lower() in {'.jpg', '.jpeg', '.png'}])\n                if frame_paths:\n                    self.samples.append((frame_paths, self.class_to_idx[cls]))\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def _select_indices(self, total):\n        if total <= 0:\n            raise ValueError(\"No frames\")\n        if total == 1:\n            return torch.zeros(self.num_frames, dtype=torch.long)\n        steps = max(self.num_frames * self.frame_stride, self.num_frames)\n        grid = torch.linspace(0, total - 1, steps=steps)\n        idxs = grid[::self.frame_stride].long()\n        if idxs.numel() < self.num_frames:\n            pad = idxs.new_full((self.num_frames - idxs.numel(),), idxs[-1].item())\n            idxs = torch.cat([idxs, pad], dim=0)\n        return idxs[:self.num_frames]\n    \n    def __getitem__(self, idx):\n        frame_paths, label = self.samples[idx]\n        total = len(frame_paths)\n        idxs = self._select_indices(total)\n        frames = []\n        for i in idxs:\n            path = frame_paths[int(i.item())]\n            with Image.open(path) as img:\n                img = img.convert(\"RGB\")\n                frames.append(self.to_tensor(img))\n        video = torch.stack(frames)\n        video = self.transform(video)\n        return video, label\n\n\nclass TestDataset(Dataset):\n    def __init__(self, root, num_frames=16, frame_stride=2, image_size=224):\n        self.root = Path(root)\n        self.num_frames = num_frames\n        self.frame_stride = frame_stride\n        self.transform = VideoTransform(image_size, is_train=False)\n        self.to_tensor = transforms.ToTensor()\n        self.video_dirs = sorted([d for d in self.root.iterdir() if d.is_dir()], key=lambda x: int(x.name))\n        self.video_ids = [int(d.name) for d in self.video_dirs]\n    \n    def __len__(self):\n        return len(self.video_dirs)\n    \n    def _select_indices(self, total):\n        if total <= 0:\n            raise ValueError(\"No frames\")\n        if total == 1:\n            return torch.zeros(self.num_frames, dtype=torch.long)\n        steps = max(self.num_frames * self.frame_stride, self.num_frames)\n        grid = torch.linspace(0, total - 1, steps=steps)\n        idxs = grid[::self.frame_stride].long()\n        if idxs.numel() < self.num_frames:\n            pad = idxs.new_full((self.num_frames - idxs.numel(),), idxs[-1].item())\n            idxs = torch.cat([idxs, pad], dim=0)\n        return idxs[:self.num_frames]\n    \n    def __getitem__(self, idx):\n        video_dir = self.video_dirs[idx]\n        video_id = self.video_ids[idx]\n        frame_paths = sorted([p for p in video_dir.iterdir() if p.suffix.lower() in {'.jpg', '.jpeg', '.png'}])\n        total = len(frame_paths)\n        idxs = self._select_indices(total)\n        frames = []\n        for i in idxs:\n            path = frame_paths[int(i.item())]\n            with Image.open(path) as img:\n                img = img.convert(\"RGB\")\n                frames.append(self.to_tensor(img))\n        video = torch.stack(frames)\n        video = self.transform(video)\n        return video, video_id\n\nprint(\"Dataset classes defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:17:05.480750Z","iopub.execute_input":"2026-01-10T12:17:05.481261Z","iopub.status.idle":"2026-01-10T12:17:05.496231Z","shell.execute_reply.started":"2026-01-10T12:17:05.481238Z","shell.execute_reply":"2026-01-10T12:17:05.495563Z"}},"outputs":[{"name":"stdout","text":"Dataset classes defined\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"full_dataset = VideoDataset(PATH_DATA_TRAIN, num_frames=NUM_FRAMES, frame_stride=FRAME_STRIDE, image_size=IMG_SIZE, is_train=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:30:49.384316Z","iopub.execute_input":"2026-01-10T12:30:49.384990Z","iopub.status.idle":"2026-01-10T12:30:57.320976Z","shell.execute_reply.started":"2026-01-10T12:30:49.384963Z","shell.execute_reply":"2026-01-10T12:30:57.320188Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_size = int(0.9 * len(full_dataset))\nval_size = len(full_dataset) - train_size\ntrain_ds, val_ds = random_split(full_dataset, [train_size, val_size], generator = generator)\nval_ds.dataset.is_train = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:38:28.843980Z","iopub.execute_input":"2026-01-10T12:38:28.844690Z","iopub.status.idle":"2026-01-10T12:38:28.848977Z","shell.execute_reply.started":"2026-01-10T12:38:28.844664Z","shell.execute_reply":"2026-01-10T12:38:28.848168Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size = BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:43:15.753337Z","iopub.execute_input":"2026-01-10T12:43:15.753600Z","iopub.status.idle":"2026-01-10T12:43:15.757666Z","shell.execute_reply.started":"2026-01-10T12:43:15.753582Z","shell.execute_reply":"2026-01-10T12:43:15.757076Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"print(f\"Train samples: {len(train_ds)}\")\nprint(f\"Classes: {len(train_ds.dataset.classes)}\")\nprint(f\"Class names: {train_ds.dataset.classes[:10]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:43:17.373002Z","iopub.execute_input":"2026-01-10T12:43:17.373538Z","iopub.status.idle":"2026-01-10T12:43:17.377647Z","shell.execute_reply.started":"2026-01-10T12:43:17.373517Z","shell.execute_reply":"2026-01-10T12:43:17.376839Z"}},"outputs":[{"name":"stdout","text":"Train samples: 5628\nClasses: 51\nClass names: ['brush_hair', 'cartwheel', 'catch', 'chew', 'clap', 'climb', 'climb_stairs', 'dive', 'draw_sword', 'dribble']...\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"batch = next(iter(train_loader))\nvideo, label = batch\n\nprint(\"Train video shape:\", video.shape)\nprint(\"Train label shape:\", label.shape)\nprint(\"Train video dtype:\", video.dtype)\nprint(\"Train labels:\", label)\n\nbatch = next(iter(val_loader))\nvideo, label = batch\n\nprint(\"Validation video shape:\", video.shape)\nprint(\"Validation label shape:\", label.shape)\nprint(\"Validation video dtype:\", video.dtype)\nprint(\"Validation labels:\", label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:44:55.903147Z","iopub.execute_input":"2026-01-10T12:44:55.903716Z","iopub.status.idle":"2026-01-10T12:45:04.850810Z","shell.execute_reply.started":"2026-01-10T12:44:55.903686Z","shell.execute_reply":"2026-01-10T12:45:04.849830Z"}},"outputs":[{"name":"stdout","text":"Train video shape: torch.Size([16, 16, 3, 224, 224])\nTrain label shape: torch.Size([16])\nTrain video dtype: torch.float32\nTrain labels: tensor([45, 17, 42,  9, 21, 45, 45, 12, 41, 28,  2, 20, 29, 41, 14, 29])\nValidation video shape: torch.Size([16, 16, 3, 224, 224])\nValidation label shape: torch.Size([16])\nValidation video dtype: torch.float32\nValidation labels: tensor([49, 48, 45, 23,  9,  5, 49,  2, 49,  1, 34,  1, 37, 42, 33, 32])\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"class VideoClassifier(nn.Module):\n    def __init__(self, num_classes, backbone=\"resnet50\", dropout=0.5):\n        super().__init__()\n\n        if backbone == \"resnet18\":\n            net = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n            feat_dim = 512\n        elif backbone == \"resnet50\":\n            net = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n            feat_dim = 2048\n        else:\n            raise ValueError(\"Unsupported backbone\")\n\n        # Remove final FC\n        self.backbone = nn.Sequential(*list(net.children())[:-1])\n\n        self.temporal_pool = nn.AdaptiveAvgPool1d(1)\n\n        self.classifier = nn.Sequential(\n            nn.Linear(feat_dim, feat_dim // 2),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(feat_dim // 2, num_classes)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        x: [B, T, C, H, W]\n        \"\"\"\n        B, T, C, H, W = x.shape\n        x = x.view(B * T, C, H, W)\n\n        feats = self.backbone(x)           # [B*T, D, 1, 1]\n        feats = feats.flatten(1)           # [B*T, D]\n\n        feats = feats.view(B, T, -1)        # [B, T, D]\n        feats = feats.transpose(1, 2)       # [B, D, T]\n\n        video_feat = self.temporal_pool(feats).squeeze(-1)  # [B, D]\n\n        out = self.classifier(video_feat)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:14:30.001824Z","iopub.execute_input":"2026-01-10T13:14:30.002565Z","iopub.status.idle":"2026-01-10T13:14:30.009018Z","shell.execute_reply.started":"2026-01-10T13:14:30.002540Z","shell.execute_reply":"2026-01-10T13:14:30.008173Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"# Replace this import with whatever VideoMAE implementation you prefer\nfrom timm.models.video_mae import video_mae_base_patch16_224\n\nclass VideoMAEForAction(nn.Module):\n    def __init__(self, num_classes=51, pretrained=True):\n        super().__init__()\n\n        # Load VideoMAE backbone\n        # `video_mae_base_patch16_224` is from timm\n        # It returns per-frame features if configured that way\n        self.backbone = video_mae_base_patch16_224(\n            pretrained=pretrained,\n            num_classes=0  # remove classification head\n        )\n\n        # Dimension of the backbone features\n        embed_dim = self.backbone.embed_dim  # usually 768 for base\n\n        # Classification head\n        self.classifier = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim//2),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(embed_dim//2, num_classes)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        x: [B, T, C, H, W]\n        \"\"\"\n\n        # VideoMAE expects a batch of videos\n        # It returns features of shape [B, T, EMBED_DIM]\n        feats = self.backbone.forward_features(x)\n\n        # Average over time\n        # feats: [B, T, EMBED_DIM]\n        # pooled: [B, EMBED_DIM]\n        pooled = feats.mean(dim=1)\n\n        # Final classification\n        out = self.classifier(pooled)\n\n        return out\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LightningClassifier(pl.LightningModule):\n    def __init__(self, model, num_classes = 51, lr=BASE_LR, weight_decay=WEIGHT_DECAY):\n        super().__init__()\n        self.save_hyperparameters(ignore=[\"model\"])\n\n        self.model = model\n        self.criterion = nn.CrossEntropyLoss()\n\n        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n        self.val_acc   = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        video, label = batch\n        logits = self(video)\n        loss = self.criterion(logits, label)\n\n        acc = self.train_acc(logits, label)\n\n        self.log(\"train/loss\", loss, prog_bar=True)\n        self.log(\"train/acc\", acc, prog_bar=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        video, label = batch\n        logits = self(video)\n        loss = self.criterion(logits, label)\n\n        acc = self.val_acc(logits, label)\n\n        self.log(\"val/loss\", loss, prog_bar=True)\n        self.log(\"val/acc\", acc, prog_bar=True)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(\n            self.parameters(),\n            lr=self.hparams.lr,\n            weight_decay=self.hparams.weight_decay\n        )\n        return optimizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:14:30.141052Z","iopub.execute_input":"2026-01-10T13:14:30.141273Z","iopub.status.idle":"2026-01-10T13:14:30.147921Z","shell.execute_reply.started":"2026-01-10T13:14:30.141256Z","shell.execute_reply":"2026-01-10T13:14:30.147355Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"model_name = \"ResNet-AvgPool-Classifier\" # CHANGE THIS WHEN USE ANOTHER MODEL\n\ncheckpoint_dir = f\"/kaggle/working/checkpoints/{model_name}\"\nckpt_path = None\n\nif os.path.exists(checkpoint_dir):\n    ckpts = [os.path.join(checkpoint_dir, f) for f in os.listdir(checkpoint_dir) if f.endswith(\".ckpt\")]\n    if ckpts:\n        ckpt_path = max(ckpts, key=os.path.getctime)  # latest file by creation time\n\nprint(\"Resuming from checkpoint:\" if ckpt_path else \"No checkpoint found.\", ckpt_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:14:30.446627Z","iopub.execute_input":"2026-01-10T13:14:30.447335Z","iopub.status.idle":"2026-01-10T13:14:30.452346Z","shell.execute_reply.started":"2026-01-10T13:14:30.447311Z","shell.execute_reply":"2026-01-10T13:14:30.451516Z"}},"outputs":[{"name":"stdout","text":"No checkpoint found. None\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"early_stop = EarlyStopping(\n    monitor=\"val/loss\",\n    patience=5,\n    mode=\"min\",\n    verbose=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:14:30.967624Z","iopub.execute_input":"2026-01-10T13:14:30.968179Z","iopub.status.idle":"2026-01-10T13:14:30.971935Z","shell.execute_reply.started":"2026-01-10T13:14:30.968156Z","shell.execute_reply":"2026-01-10T13:14:30.971147Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"checkpoint_cb = ModelCheckpoint(\n    dirpath=f'/kaggle/working/checkpoints/{model_name}',\n    monitor=\"val/loss\",\n    mode=\"min\",\n    save_top_k=1,\n    save_last=True,              # ðŸ”‘ allows resume\n    filename=\"epoch{epoch:02d}-val_loss{val/loss:.4f}\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:14:31.849745Z","iopub.execute_input":"2026-01-10T13:14:31.850296Z","iopub.status.idle":"2026-01-10T13:14:31.854704Z","shell.execute_reply.started":"2026-01-10T13:14:31.850272Z","shell.execute_reply":"2026-01-10T13:14:31.854022Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"\nwandb_logger = WandbLogger(\n    project=\"video-classification\",\n    log_model=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:14:32.203603Z","iopub.execute_input":"2026-01-10T13:14:32.204151Z","iopub.status.idle":"2026-01-10T13:14:32.207382Z","shell.execute_reply.started":"2026-01-10T13:14:32.204129Z","shell.execute_reply":"2026-01-10T13:14:32.206764Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"student_id = \"10423057\"  # TODO: replace with your student ID\napi_key = os.environ.get(\"WANDB_API_KEY\", \"83f4544a22543e319c6009abceaac90b634c68a3\")  # configure your wandb key here\n\nif api_key == \"\":\n    raise ValueError(\"Please set your wandb key in the code or in the environment variable WANDB_API_KEY\")\nelse:\n    print(\"WandB API key is set. Proceeding with login...\")\n    \nwandb.login(key=api_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:14:32.900251Z","iopub.execute_input":"2026-01-10T13:14:32.900969Z","iopub.status.idle":"2026-01-10T13:14:33.027468Z","shell.execute_reply.started":"2026-01-10T13:14:32.900945Z","shell.execute_reply":"2026-01-10T13:14:33.026924Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"WandB API key is set. Proceeding with login...\n","output_type":"stream"},{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":76},{"cell_type":"code","source":"trainer = pl.Trainer(\n    max_epochs=100,\n    accelerator=\"gpu\",\n    devices=1,\n    precision=\"16-mixed\",          # optional but recommended\n    callbacks=[early_stop, checkpoint_cb],\n    logger=wandb_logger,\n    log_every_n_steps=10\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:14:33.842823Z","iopub.execute_input":"2026-01-10T13:14:33.843648Z","iopub.status.idle":"2026-01-10T13:14:33.902796Z","shell.execute_reply.started":"2026-01-10T13:14:33.843619Z","shell.execute_reply":"2026-01-10T13:14:33.902037Z"}},"outputs":[{"name":"stderr","text":"Using 16bit Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"base_model = VideoClassifier(51) # CHANGE THIS IF USE ANOTHER BASE MODEL\nlightning_model = LightningClassifier(base_model)\ntrainer.fit(\n    lightning_model,\n    train_dataloaders=train_loader,\n    val_dataloaders=val_loader,\n    ckpt_path = ckpt_path\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:15:09.563807Z","iopub.execute_input":"2026-01-10T13:15:09.564530Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>./wandb/run-20260110_131510-8u179fkh</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/10423057-vietnamese-german-university/video-classification/runs/8u179fkh' target=\"_blank\">faithful-night-1</a></strong> to <a href='https://wandb.ai/10423057-vietnamese-german-university/video-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/10423057-vietnamese-german-university/video-classification' target=\"_blank\">https://wandb.ai/10423057-vietnamese-german-university/video-classification</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/10423057-vietnamese-german-university/video-classification/runs/8u179fkh' target=\"_blank\">https://wandb.ai/10423057-vietnamese-german-university/video-classification/runs/8u179fkh</a>"},"metadata":{}},{"name":"stderr","text":"LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n\n  | Name      | Type               | Params | Mode \n---------------------------------------------------------\n0 | model     | VideoClassifier    | 25.7 M | train\n1 | criterion | CrossEntropyLoss   | 0      | train\n2 | train_acc | MulticlassAccuracy | 0      | train\n3 | val_acc   | MulticlassAccuracy | 0      | train\n---------------------------------------------------------\n25.7 M    Trainable params\n0         Non-trainable params\n25.7 M    Total params\n102.634   Total estimated model params size (MB)\n160       Modules in train mode\n0         Modules in eval mode\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b188a831910247e7a15cd6bea56b6d43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"Metric val/loss improved. New best score: 3.324\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"Metric val/loss improved by 0.524 >= min_delta = 0.0. New best score: 2.800\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"Metric val/loss improved by 0.539 >= min_delta = 0.0. New best score: 2.261\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"Metric val/loss improved by 0.367 >= min_delta = 0.0. New best score: 1.894\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"Metric val/loss improved by 0.210 >= min_delta = 0.0. New best score: 1.684\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"print(\"\\nLoading model for testing...\")\n# Recreate base model\nbase_testing_model = VideoClassifier(num_classes=51) # CHANGE THIS IF USE ANOTHER BASE MODEL\n\n# Load Lightning model from checkpoint\nlightning_testing_model = LightningClassifier.load_from_checkpoint(\n    ckpt_path,\n    model=base_testing_model   # must pass the wrapped model\n)\n\nlightning_model.eval()\nlightning_model.to(DEVICE)\n\n# Get the actual PyTorch model\ntesting_model = lightning_model.model\ntesting_model.eval()\n\nclasses = train_ds.dataset.classes\nprint(\"\\nLoading test dataset...\")\ntest_dataset = TestDataset(PATH_DATA_TEST, num_frames=NUM_FRAMES, frame_stride=FRAME_STRIDE, image_size=IMG_SIZE)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\nprint(f\"Test samples: {len(test_dataset)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nRunning inference...\")\n\npredictions = []\n\nwith torch.no_grad():\n    for videos, video_ids in tqdm(test_loader, desc=\"Inference\"):\n        videos = videos.to(DEVICE)           # [B, T, C, H, W]\n\n        logits = testing_model(videos)               # [B, num_classes]\n        preds = logits.argmax(dim=1)         # [B]\n\n        for vid, pred_idx in zip(video_ids.cpu().numpy(),\n                                 preds.cpu().numpy()):\n            pred_class = classes[pred_idx]\n            predictions.append((vid, pred_class))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions.sort(key=lambda x: x[0])\nprint(f\"\\nTotal predictions: {len(predictions)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_path = Path(\"./submission.csv\")\n\nwith open(submission_path, \"w\") as f:\n    f.write(\"id,class\\n\")\n    for video_id, pred_class in predictions:\n        f.write(f\"{video_id},{pred_class}\\n\")\n\nprint(\"=\" * 40)\nprint(f\"Submission saved to: {submission_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}